---
title: "Ridge Regressions"
author: "Torrin Danner, Nickolajs Prihodko, Joshua Hudson, Sascha Schmalz"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE}
library(mlbench)
library(caret)
library(leaps)
```


This Vignette shows the answers to section 1.2 from Lab 7.

## Divide the BostonHousing data (or your own API data) into a test and training dataset using the caret package.

```{r}
data("BostonHousing")
trainIndex <- caret::createDataPartition(BostonHousing$age, p = .75,
                                         list = FALSE,
                                         times= 1)
datTrain <- BostonHousing[trainIndex, ]
datTest <- BostonHousing[-trainIndex, ]

head(datTrain)
head(datTest)
```

## Fit linear regression models
### Fit linear regression model

```{r}
ridge <- caret::train(crim~.,
                 data = datTrain,
                 method='lm',
                 trControl = trainControl(method = "cv")
)

print(ridge)

```

### Fit linear regression model with forward selection covariates

```{r}
ridge <- caret::train(crim~.,
                 data = datTrain,
                 method='leapForward'
)

print(ridge)

```

The fitted model shows an R2 of ... and ...

## Fit ridgereg model 
```{r, echo = FALSE}
caridge<-function(X){
  data("BostonHousing")
  dat <- X
  trainIndex <- caret::createDataPartition(dat$age, p = .75,
                                           list = FALSE,
                                           times= 1)
  datTrain <- dat[trainIndex, ]
  datTest <- dat[-trainIndex, ]
  res <- c(datTrain, datTest)
  fitControl <- caret::trainControl(method = "cv",
                             number = 10)
  # Set seq of lambda to test
  lambdaGrid <- expand.grid(lambda = c(0,.01,.02,.03,.04))
  ridge <- caret::train(crim~.,
                 data = datTrain,
                 method='ridge',
                 trControl = fitControl,
                                tuneGrid = lambdaGrid,
                 preProcess=c('center', 'scale')
  )
  predict(ridge$finalModel, type='coef', mode='norm')$coefficients[13,]
  ridge.pred <- predict(ridge, datTest)
  avgErrror<-2*sqrt(mean(ridge.pred - datTest$crim)^2)
  return(ridge)
}
```

```{r}
caridge(BostonHousing)
```
The best value of lambda for a 10-fold cross validation can be found in the output.

## Evaluation of the models

The R^2 are quite similar and depend heavily on the random selection of dividing the data into test and training. 
